from inference.ollama_inference import OllamaInference
from prompt.main_prompt import MainPrompt
from prompt.weather_prompt import WeatherPrompt
from tools.weather import WeatherAgent
import asyncio
from mcp_servers import ClientSession, StdioServerParameters
from mcp_servers.client.stdio import stdio_client
import re
from util.logger import ChobigLogger

log = ChobigLogger.get_logger()

class Main:
    def __init__(self):
        self.model_name = "command-r"
        self.main_prompt:MainPrompt = MainPrompt()
        self.weather_prompt:WeatherPrompt = WeatherPrompt()
        self.ollama_inference:OllamaInference = OllamaInference(self.model_name)

        #mcp_servers ìœ„ì¹˜ ì§€ì •
        self.weather_mcp = StdioServerParameters(
            command="python",
            args=["mcp_servers/mcp_server.py"],
            env=None
        )

    async def main_async(self, user_question):
        system_prompt = self.main_prompt.start_system_prompt()
        log.info("main_async ì§„ì… ì™„ë£Œ. system prompt get.")
        # 1. ì˜ë„ íŒë‹¨ ë‹¨ê³„
        content = self.ollama_inference.inference(system_prompt, user_question)
        log.info(f"1 íŒë‹¨ ë‹¨ê³„ ê²°ê³¼ : {content}")

        # 2. ê²€ìƒ‰ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
        if "[ë‚ ì”¨]" in content:
            match = re.search(r"\[ë‚ ì”¨\]\s*(.*)", content)
            if match:
                search_query = match.group(1).strip()
                weather_info = await self.run_mcp(search_query)
                log.info(f"ë‚ ì”¨ ë¶„ì„ ê²°ê³¼ : {weather_info}")
                # 3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë“¤ê³  ë‹¤ì‹œ ë‹µë³€ ìƒì„±
                system_prompt = self.weather_prompt.search_end_system_prompt(weather_info.content[0].text)
                log.info(f"ê²€ìƒ‰ê²°ê³¼ system prompt : {system_prompt}")
                content = self.ollama_inference.inference(system_prompt, user_question)
        return content


    async def run_mcp(self, search_keyword):
        log.info("ì´ˆë¹…ìŠ¤ async ê°€ë™!")

        async with stdio_client(self.weather_mcp) as (read, write):
            async with ClientSession(read,write) as session:
                # ì„œë²„ ì´ˆê¸°í™”
                await session.initialize()
                log.info("MCP ì„œë²„ ì—°ê²° ì„±ê³µ!")

                log.info(f"search keyword : {search_keyword}")
                result = await session.call_tool("fetch_local_weather", {"search_keyword":search_keyword})

                return result

    # def run_agent_old(self, user_question):
    #     system_prompt = self.main_prompt.start_system_prompt()
    #
    #     # 1. ì˜ë„ íŒë‹¨ ë‹¨ê³„
    #     content = self.ollama_inference.inference(system_prompt, user_question)
    #     log.info(f"1 íŒë‹¨ ë‹¨ê³„ ê²°ê³¼ : {content}")
    #
    #     # 2. ê²€ìƒ‰ ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    #     if "SEARCH:" in content:
    #         # SEARCH: ë’·ë¶€ë¶„ë§Œ ê¹”ë”í•˜ê²Œ ë”°ë‚´ê¸°
    #         search_query = content.split("SEARCH:")[1].strip().split('\n')[0]
    #         if "[ë‚ ì”¨]" in content:
    #             weather_agent = WeatherAgent(self.ollama_inference)
    #             search_query = weather_agent.run(search_query)
    #             log.info(f"ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ : {search_query}")
    #
    #         # 3. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë“¤ê³  ë‹¤ì‹œ ë‹µë³€ ìƒì„±
    #         system_prompt = self.main_prompt.end_system_prompt(search_query)
    #         log.info(f"ê²€ìƒ‰ê²°ê³¼ system prompt : {system_prompt}")
    #         final_res = self.ollama_inference.inference(system_prompt, user_question)
    #         return final_res
    #     return content


if __name__ == "__main__":
    # ê°€ë™!
    print(f"ğŸ“¡ Command-R(35B) ì—ì´ì „íŠ¸ ëŒ€ê¸° ì¤‘... (VRAM + RAM í™œìš© ëª¨ë“œ)")
    main = Main()

    async def run_chat():
        while True:
            user_input = input("[ì§ˆì˜]: ")

            if user_input.lower() in ['ì¢…ë£Œ', 'exit', 'quit']:
                log.info("ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

            if not user_input.strip():
                continue

            try:
                #ì—ì´ì „íŠ¸ì—ê²Œ ì§ˆì˜
                user_input = "ì§€ê¸ˆ ì„œìš¸ ë‚ ì”¨ ì–´ë•Œ? ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰í•´ì„œ ì•Œë ¤ì¤˜."
                ret = await main.main_async(user_input)
                print(f"\n[ì´ˆë¹…ìŠ¤ì˜ ë‹µë³€]:\n {ret} ")

            except Exception as e:
                print(f"ë£¨í”„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ! {e}")
            finally:
                print("ì™„ë£Œ!")

asyncio.run(run_chat())